---
layout: post
title: "db2数据导入导出效率对比"
date: 2015-05-14 13:52:50 +0800
comments: true
categories: db2
---
## 背景
* 生产环境的流水数据,在日终批量的时候,需要把前一天的流水数据迁移至历史库.    
* 每天两千万条流水,整个批量耗时1~2个小时.    
* 当前交易量还只是未来计划的1/4,如果达到预期的交易量,那么批量程序的耗时这么下去就太离谱了.    
* 批量程序只记录开始和结束时间,未统计或者日志中输出每个主要过程的耗时,因此只能在测试环境尝试发现耗时点和优化方法.
* 有些地方更新线上版本实在太困难(没有大的问题,能跑就坚决不动)

## 处理过程
* 当前库.流水表->卸载分区->生成临时表->导出数据
* 后线库.流水表->导入数据->生成临时表->加载分区->数据完整性校验
* 当前库.流水表->删除临时表

## 耗时分析

### 数据导出
* 这里的导出是指**[生成临时表->导出数据]**这一步
* 优化前
```
db2 "export to 文件名.ixf of ixf select * from 临时表名"
```
* 优化后
```
db2 "export to 文件名.ixf of del modified by coldel0x01 select * from 临时表名"
```
* 两种方式导出的数据文件,数据的分隔符不同(再深入的差别不晓得)
* 在测试环境,针对优化前后进行了测试 

|  项目 |  数据量  |  导出耗时  |  导出文件  | 
|----- | ------- | --------- :|  ------ :|   
|优化前 | 1000万 | 10分钟 | 8G | 
|优化后 | 1000万 | 2分钟 | 4G  |

### 数据导入
* 导入是指**[导入数据->生成临时表]**这一步
* 优化前
```
db2 "import from 文件名.ixf of ixf commitcount 500 create into 临时表名 in 表空间"
```
* 优化后
```
db2 "create table 临时表名 like 表名 in 表空间"
db2 "load from 文件名.ixf of ixf insert into 临时表名 nonrecoverable"
```
* 在测试环境,针对优化前后进行了测试,该导入测试,依赖的导出数据为**优化前的导出格式(默认的分隔符)**

|  项目 |  数据量  |  导入耗时  | 
|----- | ------- | --------- :|  
|优化前 | 1000万 | too lang| 
|优化后 | 1000万 | 2分钟 |



